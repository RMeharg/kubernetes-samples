# fluentd-config-map.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    <match fluent.**>
        # This tells fluentd to not output its log on stdout
        @type null
    </match>
    <match kubernetes.var.log.containers.**kube-system**.log>
        # This tells fluentd to not output kube-system logs
        @type null
    </match>

    # Here we read the logs from Docker's containers and parse them
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>

    # we use kubernetes metadata plugin to add metadatas to the log
    <filter kubernetes.**>
        @type kubernetes_metadata
    </filter>

    # only get logs for label fluentd: "true"
    #<filter kubernetes.**>
    #    @type grep
    #    <regexp>
    #        key $.kubernetes.labels.fluentd
    #        pattern true
    #    </regexp>
    #</filter>

    # only get logs for no label fluentd: "false"
    #<filter kubernetes.**>
    #    @type grep
    #    <regexp>
    #        key $.kubernetes.labels.fluentd
    #        pattern false
    #    </regexp>
    #</filter>

    # this add the namespace name at the begining of the tag
    #<match kubernetes.**>
    #    @type rewrite_tag_filter
    #    <rule>
    #        key $['kubernetes']['namespace_name']
    #        pattern ^(.+)$
    #        tag $1.${tag}
    #    </rule>
    #</match>
    #
    ## match the dev logs
    #<match dev.kubernetes.**>
    #    @type elasticsearch
    #    include_tag_key true
    #    host "#{ENV['FLUENT_ELASTICSEARCH_HOST_DEV']}"
    #    port "#{ENV['FLUENT_ELASTICSEARCH_PORT_DEV']}"
    #    scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME_DEV'] || 'http'}"
    #    ...
    #</match>
    ## match the production logs
    #<match production.kubernetes.**>
    #    @type elasticsearch
    #    include_tag_key true
    #    host "#{ENV['FLUENT_ELASTICSEARCH_HOST_PROD']}"
    #    port "#{ENV['FLUENT_ELASTICSEARCH_PORT_PROD']}"
    #    scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME_PROD'] || 'http'}"
    #    ...
    #</match>

    # we send the logs to Elasticsearch
    <match kubernetes.**>
        @type elasticsearch
        include_tag_key true
        host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
        port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
        scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
        ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
        # user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
        # password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
        reload_connections true
        logstash_prefix logstash
        logstash_format true
        <buffer>
            flush_thread_count 8
            flush_interval 5s
            chunk_limit_size 2M
            queue_limit_length 32
            retry_max_interval 30
            retry_forever true
        </buffer>
    </match>
